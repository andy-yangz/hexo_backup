# 论文笔记：LSTM: A Search Space Odyssey

1. #### 文章有何贡献？

  是第一个对LSTM还有它的八种变体进行大规模调查的论文。

2. #### 本文研究的问题有何价值？

  本文填补了对LSTM的各组件进行系统性调查的空缺，而且一定程度上系统性的解决了如何提高LSTM架构性能的开放性问题。也为之后的研究者如何更好的使用LSTM，理解LSTM，已经调节其参数提供了很好的建议。

3. #### 本文概要

![](http://upload-images.jianshu.io/upload_images/4787675-ceab00ad6f9d87bd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

标准的LSTM主要是由这几个部分，三个门：input gate、 forget gate、 output gate， 还有输入块与输出块。
首先调查的是对这几个门进行修改的变体：
NIG: No Input Gate 没有输入门 
NFG: No Forget Gate 没有遗忘门
NOG: No Output Gate 没有输出门
NIAF: No Input Activation Function 没有输入块激活函数
NOAF: No Output Activation Function 没有输出块的激活函数
CIFG: Coupled Input and Forget Gate 耦合的输入和遗忘门
还有就是两个在LSTM上的小技巧，一个是Peephole（窥视孔），利用cell的状态来给予几个门更多的提示；还有一个是全门循环连接，也就是说通常的门是包含在LSTM单元里面输出端不直接和下一个单元交流的，但是全门连接表示将每个单元之间的门也直接连接起来。
NP: No Peephole 没有窥视孔
FGR: Full Gate Recurrence 全门循环连接

加上标准LSTM总共9个。在三个任务上进行测试，分别是：语言识别，手写识别，还有复调音乐模型建立。
对于每个模型还要对以下几个参数进行探索：LSTM的隐藏单元数，学习率，动量 (momentum)，高斯输入噪音。

4. #### 结果

  一、各变体的比较。
  标准LSTM结构在各个数据集上都能表现出较好的结果，而各个变体也并没有显著地提高了LSTM的性能。
  还有即使加入了输入门和遗忘门的耦合，或是移除了窥视孔设置，也并不会很大程度上降低性能，同时这两个设置还减少了LSTM的计算量。
  根据实验数据，还得出LSTM中，遗忘门和输出激活函数是最关键的组件。
  二、各参数的影响
  学习率是最关键的超参数，然后是网络的大小。而让人惊讶的是，动量在这几个试验中并没有很重要。
  还有高斯噪音的加入，根据任务的不同有时有帮助，有时却会有害。
  在对超参数之间的互相作用进行调查时，它们并没有表现出明显的结构性关系，甚至可以忽略其的影响。所以我们可以假设这些超参数是大概相对独立的。
  还有一个关于学习率的建议，对于一个数据集，可以先用一个小的网络找到一个好的学习率，然后把它用到大的网络中去。

5. #### 我的评价

  虽然在这篇论文中得到了关于LSTM的很多建议，但根据自己的实际实验，却有一些是和它的结论不怎么吻合的。
  首先是我之前有碰到过，变大网络反而使得性能变差，而这篇论文中是说只会提高性能。
  还有关于超参数之间的关系，我也有碰到分别调两个都可以提高性能，但是一结合就降低了性能的。
  最后一点建议是，这篇论文虽然对输入噪音进行了调查，却没有对RNN的dropout进行调查，可能这个也能作为一个探索点。
  看完本文之后还是觉得LSTM有很多很多谜团。